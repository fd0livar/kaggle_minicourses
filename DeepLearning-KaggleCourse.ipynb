{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning  \n",
    "*Use TensorFlow to take machine learning to the next level*\n",
    "\n",
    "# 1. Intro to DL for Computer Vision  \n",
    "*A quick overview of how models work on images*\n",
    "\n",
    "- Convolutions are the basic building block for deep learning models in computer vision and many other applications.  \n",
    "- Convolutions are filters that can are applied onto the images to obtain desired results. They can be: horizontal line sensors, vertical line sensors, round edges sensors, and so on.  \n",
    "- An image is represented by a tensor (generalized matrix, specially for color images). A convolution is also a tensor. Thus applying a convolution is tensor algebra. \n",
    "- Specifically, the math involved to obtain the result after applying a convolution is a straigthforward term-by-term multiplication followed by a summation. \n",
    "\n",
    "\n",
    "- Convolutions don't have to be square tensors. You can have a $4x7$ convolutions wo problem, for instance.\n",
    "\n",
    "# 2. Building Models From Convolutions\n",
    "*Scale up from simple building blocks to models with beyond human capabilities*\n",
    "\n",
    "- You can apply a series of filters (convolutions) in what is called a *layer*. \n",
    "- The resulting group of images after the application of the layer is a 3D tensor. Where the 3rd dimension is coming from all the filters that form the layer. The so called **channel dimension**\n",
    "- You can continue applying more layer to find more details from a picture\n",
    "\n",
    "\n",
    "# 3. TensorFlow Programming  \n",
    "*Start writing code using TensorFlow and Keras*\n",
    "\n",
    "We used the ResNet model and the VGG16 model, with pre-trained weights.\n",
    "\n",
    "# 4. Transfer Learning\n",
    "*A powerful technique to build highly accurate models even with limited data*\n",
    "\n",
    "- Transfer learning is useful to build new models that classify data into new categories but using an already trained model.\n",
    "- A model consists os several layers of filters plus a final layers that makes the prediction. When using Transfer Learning, that prediction layer is dropped and a *new prediction layer* is trained. \n",
    "- This new predictions layer has connections with all the nodes of the final layer of the filters (which has to be a vector) creating a so called *dense layer*. \n",
    "- The prediction values are then transformed into probabilities with a *softmax* function\n",
    "- After the model is specified, the compilation is done. The optimizer function used in the example is `sgd`: *stochastic gradient descent*. The loss function is `categorical_crossentropy`, and the metrics is `accuracy`.\n",
    " - **optimizer** determines how we determine the numerical values that make up the model. So it can affect the resulting model and predictions\n",
    " - **loss** determines what goal we optimize when determining numerical values in the model. So it can affect the resulting model and predictions\n",
    " - **metrics** determines only what we print out while the model is being built, but it doesn't affect the model itself.\n",
    "\n",
    "- After compilation comes the Fit of the model. \n",
    "- Here you have to create a train generator and a validation generator by using already classified images. \n",
    "- It is a very useful technique since it is possible to obtain high acciracy even with little new data. \n",
    "\n",
    "## Notes from exercises:\n",
    "\n",
    "- Specifying and Compiling the model when doing Transfer Learning is not computationally expensive as no new data has been introduced to the model. \n",
    "- In the Fitting step, the new data is introduced and the process takes more time.\n",
    "- when defining the *training generator* keep in mind the amount of fitted data to match the batch size and the epochs. \n",
    "- `steps_per_epochs` is a parameter of the `fit_generator` method of the model\n",
    "\n",
    "Fitting stats are:  \n",
    "`22/22 [==============================] - 24s 1s/step - loss: 0.4899 - accuracy: 0.7864 - val_loss: 0.3341 - val_accuracy: 0.8750`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 5. Data Augmentation\n",
    "*Learn a simple trick that effectively increases amount of data available for model training*\n",
    "\n",
    "There are a few parameters that can be used when generating images from the preprocessing Function `ImageDataGenerator` that increase the amount of images you can use for your training step. Some of these parameters are: `horizontal_flip` (*boolean*), to get mirror images of the original data; `width_shift_range` (and `height_...`) (*float from 0 to 1*), to shift the position of the image which will result in a slightly different image, as well. \n",
    "\n",
    "- When using Data Augmentation it is best to create 2 image generators, with and without augmentation. This way you test your model using the one without augmentation to have a standard model. \n",
    "- Another parameter for data augmentation is `rotation_range`\n",
    "- Data augmentation affects images without touching the labels, so you have to be careful when using it!\n",
    "\n",
    "Here is some example code with added data augmentation parameters\n",
    "\n",
    "```python\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, GlobalAveragePooling2D\n",
    "\n",
    "num_classes = 2\n",
    "resnet_weights_path = '../input/resnet50/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
    "\n",
    "my_new_model = Sequential()\n",
    "my_new_model.add(ResNet50(include_top=False, pooling='avg', weights=resnet_weights_path))\n",
    "my_new_model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "my_new_model.layers[0].trainable = False\n",
    "\n",
    "my_new_model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "image_size = 224\n",
    "\n",
    "# Specify the values for all arguments to data_generator_with_aug.\n",
    "data_generator_with_aug = ImageDataGenerator(preprocessing_function=preprocess_input,\n",
    "                                              horizontal_flip = True,\n",
    "                                              width_shift_range = 0.1,\n",
    "                                              height_shift_range = 0.1)\n",
    "            \n",
    "data_generator_no_aug = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "\n",
    "\n",
    "# Specify which type of ImageDataGenerator above is to load in training data\n",
    "train_generator = data_generator_with_aug.flow_from_directory(\n",
    "        directory = '../input/dogs-gone-sideways/images/train',\n",
    "        target_size=(image_size, image_size),\n",
    "        batch_size=12,\n",
    "        class_mode='categorical')\n",
    "\n",
    "# Specify which type of ImageDataGenerator above is to load in validation data\n",
    "validation_generator = data_generator_no_aug.flow_from_directory(\n",
    "        directory = '../input/dogs-gone-sideways/images/val',\n",
    "        target_size=(image_size, image_size),\n",
    "        class_mode='categorical')\n",
    "\n",
    "my_new_model.fit_generator(\n",
    "        train_generator,# if you don't know what argument goes first, try the hint\n",
    "        epochs = 3,\n",
    "        steps_per_epoch=19,\n",
    "        validation_data=validation_generator)\n",
    "```\n",
    "\n",
    "### How could you test whether data augmentation improved your model accuracy?\n",
    "\n",
    "Create `train_generator` usng `data_generator_no_aug` but don't change other arguments to `train_generator`.\n",
    "\n",
    "Run the model and see the resuling accuracy. Compare this to the accuracy you got when train_generator used augmentation.\n",
    "\n",
    "Our validation dataset is very small, so there's a little bit of luck or randomness in the exact score from any model run. Validation scores will be more reliable as you start using larger datasets.\n",
    "\n",
    "# 6. A Deeper Understanding of Deep Learning  \n",
    "*How Stochastic Gradient Descent and Back-Propagation train your deep learning model*\n",
    "\n",
    "- *Dense and Convolutional Layers*\n",
    "- The better the weights, the better the predictions\n",
    "- Loss, gradient-descent and backwards-propagation are the steps to get better weights\n",
    "- You want to minimize the loss function -> get predictions closer to target \n",
    "- Back-propagation calculates for how much the weights have to be changed\n",
    "- learning rate is a scaling factor to those weights changes\n",
    "\n",
    "# 7. Deep Learning From Scratch  \n",
    "*Build models without transfer learning. Especially important for uncommon image types.*\n",
    "\n",
    "- You have to define the hidden layers as well as the predictions layer. \n",
    "- The most used activation function for the hidden layers is the [RELU](https://www.kaggle.com/dansbecker/rectified-linear-units-relu-in-deep-learning)\n",
    "- The output of the hidden layers has to be one-dimensional, we thus **Flatten()** them out before the prediction layer. \n",
    "- These model perform better when you add a Dense layer before the precition layer.\n",
    "- You fit the model using your data, and you can define the ratio of data saved as validation data. \n",
    "- We you have all your data stored in arrays, you can fit the model with a `fit()` function directly, instead of the `fit_generator()` function used when data was created with the `ImageDataGenerator` class (?)\n",
    "\n",
    "\n",
    "Sample code to create a model from scratch\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.python import keras\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense, Flatten, Conv2D, Dropout\n",
    "\n",
    "\n",
    "img_rows, img_cols = 28, 28\n",
    "num_classes = 10\n",
    "\n",
    "def data_prep(raw):\n",
    "    out_y = keras.utils.to_categorical(raw.label, num_classes)\n",
    "\n",
    "    num_images = raw.shape[0]\n",
    "    x_as_array = raw.values[:,1:]\n",
    "    x_shaped_array = x_as_array.reshape(num_images, img_rows, img_cols, 1)\n",
    "    out_x = x_shaped_array / 255\n",
    "    return out_x, out_y\n",
    "\n",
    "train_file = \"../input/digit-recognizer/train.csv\"\n",
    "raw_data = pd.read_csv(train_file)\n",
    "\n",
    "x, y = data_prep(raw_data)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(20, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=(img_rows, img_cols, 1)))\n",
    "model.add(Conv2D(20, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(x, y,\n",
    "          batch_size=128,\n",
    "          epochs=2,\n",
    "          validation_split = 0.2)\n",
    "```\n",
    "\n",
    "# 8. Dropout and Strides for Larger Models  \n",
    "*Make your models faster and reduce overfitting*\n",
    "\n",
    "- Stride lengths to make your model faster and reduce memory consumption\n",
    "- Dropout to combat overfitting\n",
    "\n",
    "Both of these techniques are especially useful in large models.\n",
    "\n",
    "- Stride is a change in the *steps* made by the convolution when being applied to the data. If set to 2, then the filter moves 2 pixels at a time, instead of one. \n",
    "- This causes the resulting output size to be smaller (1/2 * 1/2)\n",
    "- As the representation going to the next latyer is smaller, the whole model is much faster\n",
    "- Another technique used to reduce output size is *max pooling*, but changing the stride length is more effective \n",
    "- Dropout consists on dropping nodes or convolutions in some part of the training. The dropped nodes are randomly selected. \n",
    "- It prevents the possible domination of one node throughout the whole training.\n",
    "- You set the ratio of dropped convolutions as a float between 0 and 1. \n",
    "- This method is very effective to prevent overfitting. \n",
    "- Before Dropout was used, peopled reduced the number of layers or convolutions to fights overfitting.\n",
    "\n",
    "Sample code using these techniques:\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.python import keras\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense, Flatten, Conv2D, Dropout\n",
    "\n",
    "img_rows, img_cols = 28, 28\n",
    "num_classes = 10\n",
    "\n",
    "def data_prep(raw):\n",
    "    out_y = keras.utils.to_categorical(raw.label, num_classes)\n",
    "\n",
    "    num_images = raw.shape[0]\n",
    "    x_as_array = raw.values[:,1:]\n",
    "    x_shaped_array = x_as_array.reshape(num_images, img_rows, img_cols, 1)\n",
    "    out_x = x_shaped_array / 255\n",
    "    return out_x, out_y\n",
    "\n",
    "train_size = 30000\n",
    "train_file = \"../input/digit-recognizer/train.csv\"\n",
    "raw_data = pd.read_csv(train_file)\n",
    "\n",
    "x, y = data_prep(raw_data)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(30, kernel_size=(3, 3),\n",
    "                 strides=2,\n",
    "                 activation='relu',\n",
    "                 input_shape=(img_rows, img_cols, 1)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Conv2D(30, kernel_size=(3, 3), strides=2, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(x, y,\n",
    "          batch_size=128,\n",
    "          epochs=2,\n",
    "          validation_split = 0.2)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
